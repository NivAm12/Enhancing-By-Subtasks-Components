{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, EvalPrediction, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "# import wandb\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_with_head = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "pre_trained_model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert a layer in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier_layer(layer_to_insert):\n",
    "    till_layer_to_insert = pre_trained_model.encoder.layer[:layer_to_insert]\n",
    "    after_layer_to_insert = pre_trained_model.encoder.layer[layer_to_insert:]\n",
    "    model_with_middle_classifier = till_layer_to_insert + [nn.Linear(768, 2)] + after_layer_to_insert\n",
    "\n",
    "    return model_with_middle_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-3): 4 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (4): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (5-12): 8 x BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (intermediate_act_fn): GELUActivation()\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_middle_classifier = create_classifier_layer(layer_to_insert=4)\n",
    "model_with_middle_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/acronym_data.txt'\n",
    "data = []\n",
    "\n",
    "with open(file_path, \"r\", errors='ignore') as file:\n",
    "    for line in file.readlines():\n",
    "        split = line.strip().split('|')\n",
    "        \n",
    "        # build the sentence structure\n",
    "        source_sentence = split[6]\n",
    "        compare_sentence = source_sentence[:int(split[3])] + split[1] + source_sentence[int(split[4]):]\n",
    "\n",
    "        row = {\n",
    "            'source_sentence': source_sentence,\n",
    "            'compare_sentence': compare_sentence,\n",
    "            'label': 1\n",
    "        }\n",
    "        data.append(row)\n",
    "\n",
    "data_dict = {key: [item[key] for item in data] for key in data[0]}\n",
    "dataset = Dataset.from_dict(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    41,     9,     7,  9533,     7,     9,    41,    41,     9,\n",
       "             7,  9533,     7,     9,    41,  1977,    43,  3230,    17,  2476,\n",
       "            17,  4156, 12413,  9532,    23,    16,  8202,    22,    17,    20,\n",
       "            17,    20,    17,    22,    16,  2746,  3549,  1942,  1920,  7877,\n",
       "          4921,  6581,  2465,  1927,  3605, 10120,  7633,  3328,  3993,    26,\n",
       "            43,    18,    55,    18,  1920,  2774,  3731,  2162,    43,  3171,\n",
       "          4403,  1927,  4537,  2430,    43, 12749,  2186,    18,  4693,  2430,\n",
       "          2252,  3489,  2019,  3741,  6248,  1930,  1982, 18313,  1988,  4693,\n",
       "          2430,    43, 12749,  2186, 17002,  2256, 20898,  3368,  1036,    18,\n",
       "          1920,  2774,  2019,  1988,  2367,  2430,  2252, 22134,  1919,  1942,\n",
       "          4978,    43,    46,    10,    45,  1930,  1982,  3385, 10318,  5682,\n",
       "         21898,  1945,  1942,  4087,  4080,    43, 29670,    16,  2406,    16,\n",
       "          2774, 11390,  2019,  1988,  2367,  1942, 11660,    43,    46,    10,\n",
       "            45,  2014,  1942,  6132,  1920,  5682, 21898,  1945,    18,     3,\n",
       "            41,     9,     7,  9533,     7,     9,    41,    41,     9,     7,\n",
       "          9533,     7,     9,    41,  1977,    43,  3230,    17,  2476,    17,\n",
       "          4156, 12413,  9532,    23,    16,  8202,    22,    17,    20,    17,\n",
       "            20,    17,    22,    16,  2746,  3549,  1942,  1920,  7877,  4921,\n",
       "          6581,  2465,  1927,  3605, 10120,  7633,  3328,  3993,    26,    43,\n",
       "            18,    55,    18,  1920,  2774,  3731,  2162,    43,  3171,  4403,\n",
       "          1927,  4537,  2430,    43, 12749, 15660,    18,  4693,  2430,  2252,\n",
       "          3489,  2019,  3741,  6248,  1930,  1982, 18313,  1988,  4693,  2430,\n",
       "            43, 12749,  2186, 17002,  2256, 20898,  3368,  1036,    18,  1920,\n",
       "          2774,  2019,  1988,  2367,  2430,  2252, 22134,  1919,  1942,  4978,\n",
       "            43,    46,    10,    45,  1930,  1982,  3385, 10318,  5682, 21898,\n",
       "          1945,  1942,  4087,  4080,    43, 29670,    16,  2406,    16,  2774,\n",
       "         11390,  2019,  1988,  2367,  1942, 11660,    43,    46,    10,    45,\n",
       "          2014,  1942,  6132,  1920,  5682, 21898,  1945,    18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(dataset[0]['source_sentence'], dataset[0]['compare_sentence'], return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadPubMedBert(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, classifier_heads: list):\n",
    "        super(MultiHeadPubMedBert, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
    "        self.heads = classifier_heads\n",
    "\n",
    "    def forward(self, tokens, head_to_use):\n",
    "        outputs = self.base_model(**tokens)\n",
    "        outputs = self.heads[head_to_use](outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinariHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinariHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
    "        self.mean = torch.mean\n",
    "        self.linear = nn.Linear(in_features=768, out_features=2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x[0])\n",
    "        x = self.mean(x, dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MultiClassHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
    "        self.mean = torch.mean\n",
    "        self.linear = nn.Linear(in_features=768, out_features=4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x[0])\n",
    "        x = self.mean(x, dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [BinariHead(), MultiClassHead()]\n",
    "multi_head_model = MultiHeadPubMedBert(pre_trained_model, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5180, 0.5014]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = multi_head_model(tokens, 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5240, 0.4672, 0.4792, 0.5236]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = multi_head_model(tokens, 1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
