{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "from data.acronymDataset import AcronymDataset\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=config.max_position_embeddings)\n",
    "pre_trained_model = AutoModel.from_pretrained(model_name).to('mps')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset already been loaded, using the cached dataset..\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "file_path = 'data/acronym_data.txt'\n",
    "dataset = AcronymDataset(file_path=file_path, tokenizer=tokenizer)\n",
    "data = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/37331 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset.preprocss_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from models.multiHeadModel import MultiHeadModel\n",
    "from models.heads import ClassificationHead\n",
    "\n",
    "in_features = config.hidden_size\n",
    "binari_head = ClassificationHead(in_features=in_features, out_features=1).to('mps')\n",
    "four_labels_head = ClassificationHead(in_features=in_features, out_features=4).to('mps')\n",
    "\n",
    "classifiers = torch.nn.ModuleDict({\n",
    "    \"binari_head\": binari_head,\n",
    "    # \"four_labels_head\": four_labels_head\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_model = MultiHeadModel(pre_trained_model, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train\n",
    "train_loader1, _ = dataset.get_dataloaders(train_size=0.9, batch_size=8)\n",
    "# train_loader2, _ = dataset.get_dataloaders(train_size=0.9, batch_size=32)\n",
    "\n",
    "optim = torch.optim.AdamW(multi_head_model.parameters(), lr=0.001)\n",
    "\n",
    "train_args = {\n",
    "    \"epochs\": 1,\n",
    "    \"device\": \"mps\",\n",
    "    \"optim\": optim\n",
    "}\n",
    "\n",
    "heads_props = {\n",
    "    \"binari_head\": {\n",
    "        \"train_loader\": train_loader1,\n",
    "        \"loss_weight\": 1.0,\n",
    "        \"loss_func\": torch.nn.BCEWithLogitsLoss()\n",
    "    },\n",
    "    # \"four_labels_head\": {\n",
    "    #     \"train_loader\": train_loader2,\n",
    "    #     \"loss_weight\": 0.8\n",
    "\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6596, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6061, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6616, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7258, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5480, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7695, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7753, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7227, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4851, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5486, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6665, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5475, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4904, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6073, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5420, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7202, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7213, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6646, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5508, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7327, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6052, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5415, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6436, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5406, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6064, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7335, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6547, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6670, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6651, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5974, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7232, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6052, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6112, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6554, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6014, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6082, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7179, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5493, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5504, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5411, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6078, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.8406, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7114, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6542, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4883, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6701, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6100, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7277, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6077, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7207, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.4858, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6006, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6644, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5513, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6133, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5383, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6603, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5530, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6001, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6025, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5985, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7348, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7263, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6010, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5465, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7152, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6071, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5422, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6714, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6625, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7161, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7241, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5400, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7116, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6726, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6562, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6085, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5440, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6548, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6083, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6117, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5478, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6578, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6711, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5968, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7228, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5482, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7209, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7182, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6130, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7733, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5974, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5949, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7193, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5957, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6634, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6586, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6630, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6524, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.5417, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6100, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7197, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [03:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(multi_head_model, heads_props, train_args)\n",
      "File \u001b[0;32m~/Desktop/files/development/Enhancing-By-Subtasks-Components/utils/train.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(multi_head_model, heads_props, train_args)\u001b[0m\n\u001b[1;32m     28\u001b[0m task_batch \u001b[39m=\u001b[39m task_batch\u001b[39m.\u001b[39mto(train_args[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[39m# loss \u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m output \u001b[39m=\u001b[39m multi_head_model(task_batch, head_name)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m critic(output\u001b[39m.\u001b[39msqueeze(), task_batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/files/development/Enhancing-By-Subtasks-Components/models/multiHeadModel.py:28\u001b[0m, in \u001b[0;36mMultiHeadModel.forward\u001b[0;34m(self, tokens, head_to_use)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tokens, head_to_use: \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(input_ids\u001b[39m=\u001b[39;49mtokens[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mtokens[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     29\u001b[0m                               token_type_ids\u001b[39m=\u001b[39;49mtokens[\u001b[39m'\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[head_to_use](outputs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:357\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    359\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(multi_head_model, heads_props, train_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
